{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Consider a first-order ordinary differential equation (ODE) $$u'(t) = f[t, u(t)]$$ For concrete examples, consider $u'(t) = 2 u(t)$, or $u'(t) = 2t^2u(t)$. Such an equation states that the function $f[t, u(t)]$ is the slope of $u(t)$ at the domain point $t$; to calculate this slope, one generally needs to know both the range value $u(t)$ as well as the domain point $t$. The usual task of solving such a differential equation is to start from an \"initial condition\" $u(t_0) = u_0$ and reconstruct what the entire function $u(t)$ must be for $t>t_0$. There are analytical methods for cleverly reconstructing such a solution (e.g., solving $u'(t) = 2u(t)$ yields $u(t) = u_0 \\exp(2 t)$, which you can verify by computing its derivative and plugging it back into the original equation), but we are interesting here in how to numerically obtain the solution without using analytical methods.\n",
    "\n",
    "To construct a solution, we first define a discrete mesh of domain points $(t_0, t_1, t_2, ..., t_N)$ with uniform spacing $\\Delta t = t_{k+1} - t_k$. Starting from the initial condition $u(t_0) = u_0$, we then iteratively construct a sequence of matching range points $(u_0, u_1, u_2, ..., u_N)$, where $u_k = u(t_k)$, by using the slopes $u'(t_k)$ at each domain point. This procedure is tricky for two reasons: (1) computing the slopes $u'(t_k)$ generally requires knowledge of the function $u(t_k)$ that one is trying to reconstruct, leading to a chicken-and-egg problem; (2) the first-derivative $u'(t_k)$ can only produce a linear approximations to $u(t)$ around the point $t_k$, so will only approximate $u(t)$ if $\\Delta t$ is sufficiently small. To be more explicit about this latter reason, consider the Taylor expansion of $u(t)$ around a particular point $t_k$: $$u(t_k + s) = u(t_k) + u'(t_k)s + u''(t_k)s^2/2 + u'''(t_k)s^3/3! + ...$$ Generally one will need to know all higher-order derivatives of $u(t)$ at $t=t_k$ to reconstruct the function at the point $t_k + s$. However, if $s=\\Delta t$ is small, then one can approximate $(\\Delta t)^2 \\approx 0$, yielding $u(t_k + \\Delta t) = u(t_k) + u'(t_k)\\Delta t$. Thus, if one knows the slope $u'(t_k) = f[t_k, u(t_k)]$, then one obtains $u_{k+1} = u_k + \\Delta t\\, f[t_k, u_k]$, which allows one to obtain the next range point $u_{k+1}$ given only knowledge of the current range point $u_k$. We say this method (called \"Euler's method\") for obtaining $u_{k+1}$ from $u_k$ is accurate to \"1st-order in $\\Delta t$\", since the error of this approximation scales as $(\\Delta t)^2$. Observe that Euler's method corresponds to rearranging the \"forward difference\" definition of the derivative.\n",
    "\n",
    "This first-order method for obtaining the approximate sequence $(u_0, u_1, u_2, ..., u_N)$ of solution range points is not the only method. For example, consider the \"Leapfrog Method\" as an alternative. With this method, one starts from the point $t_k$, then looks both forward one point and backward one point. The Taylor expansion for each yields $u(t_k + \\Delta t) = u_{k+1} = u_k + u'(t_k)\\Delta t + u''(t_k)(\\Delta t)^2/2 + O(\\Delta t)^3$ and $u(t_k - \\Delta t) = u_{k-1} = u_k - u'(t_k)\\Delta t + u''(t_k)(\\Delta t)^2/2 + O(\\Delta t)^3$, where $O(\\Delta t)^3$ means terms of order $(\\Delta t)^3$. Note that the second-order terms have the same sign, while the first-order terms have opposite sign. This means that if we take the difference of these two expansions, we get the relation: $$u_{k+1} - u_{k-1} = 2u'(t_k)\\Delta t + O(\\Delta t)^3$$\n",
    "That is, the second-order terms cancel entirely, leaving an expression that relates the next range point $u_{k+1}$ to the preceding range point (two steps back) $u_{k-1}$ and the current slope (one step back) $u'(t_k)$, with an error that scales as $(\\Delta t)^3$ (making the method accurate to \"second-order in $\\Delta t$\"). Since this method cancels out the second-order error terms, it will produce a more accurate approximation to the solution than Euler's method for finite mesh spacings $\\Delta t$. Observe that the Leapfrog Method corresponds to rearranging the \"central difference\" definition of the derivative.\n",
    "\n",
    "The goal of this assignment is to understand what these solution methods are doing graphically, as well as several more methods outlined below. To do this, for each of the following iterative solution methods, discuss together on the white board to understand how they work. Draw an arbitrary function $u(t)$ to start. Outline a (reasonably coarse) mesh of domain points $(t_0, t_1, t_2, ...)$ and find the initial value $u_0 = u(t_0)$ on the graph. Explain to each other graphically what each method is doing to construct the sequence of range points $(u_0, u_1, u_2, ...)$. When you have reached a good understanding graphically on the board, try to explain in words what the method is doing. With your group, write a notebook ```ode_methods.ipynb``` that summarizes your conclusions in words and details what each method is doing. (No need to draw graphs in the notebook, but use equations where needed.) Note that there is no coding in this assignment yet, so focus on making a clear and readable report of the methods in your notebook. We will implement the algorithms that you derive into code next week.\n",
    "   \n",
    "\n",
    "   (Note that final increment is a weighted average of four different increments - what is each doing? Why do you suppose the middle increments are more heavily weighted?)\n",
    "\n",
    "In practice, the 4th-order Runge-Kutta method is the most popular method for solving ODEs numerically, since it nicely balances the accuracy per time step with the total number of required computations. It is usually more efficient to simply decrease the time step size using 4th-order Runge-Kutta rather than find methods of higher-order accuracy. More sophisticated approaches also dynamically resize the mesh into non-uniform spacings, but we will not explore that in this class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Euler's Method (accurate to 1st-order):\n",
    "   \n",
    "   $u_{k+1} = u_k + \\Delta t\\, f[t_k, u_k]$\n",
    "\n",
    "The Euler Method is a linear approximation that uses first order differential equations, given an inital condition $u_k$. From this we can find the rate of change of the derivative of $u_k$. This equation would be $u'(t) = f(t,u(t))$, $u(t_k)=u_k$. From this we want to plug the initial condition into our differential equation. Use the rate of change and change in time to find the change between our two points $u_k$ and $u_{k+1}$. From this calculate we are able to predict follow points in the sequence (assuming the sequence is linear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Leapfrog (Midpoint) Method (accurate to 2nd-order):\n",
    "   \n",
    "   $u_{k+1} = u_{k-1} + 2\\Delta t\\, f[t_k, u_k]$\n",
    "\n",
    "The Leapfrog Method is more accurate than Euler's Method. Specifically for quadratic functions because for this one you use the term before the initial term, while also using the slope at the initial term. This is used to estimate the term directly after the initial term. If you were to compare this to Euler's Method, you would find that Euler's approximation would farther from the desired point than that of the Leapfrog Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Heun's (Trapezoid) Method (accurate to 2nd-order):\n",
    "   \n",
    "   $\\tilde{u}_{k+1} = u_k + \\Delta t\\, f[t_k, u_k]$, \n",
    "   \n",
    "   $u_{k+1} = u_k + (\\Delta t/2)(f[t_k, u_k] + f[t_{k+1}, \\tilde{u}_{k+1}])$\n",
    "\n",
    "This one is a two step process. We begin with Euler's Method once again to find the slope of the curve. From this we find the slope at the next approximation, from the slope that we already found. Once you find that, you average the slope and impliment that at the initial point to find the point we are trying to find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. 2nd-order Runge-Kutta Method (accurate to 2nd-order):\n",
    "   \n",
    "   $u_{k+1} = u_k + K_2$, \n",
    "   \n",
    "   $K_1 = \\Delta t\\, f[t_k, u_k]$, \n",
    "   \n",
    "   $K_2 = \\Delta t\\, f[t_k + \\Delta t/2, u_k + K_1/2]$  \n",
    "\n",
    "The Runge-Kutta method is a 3 step process. The first part is using the first given equation above and evaluating $u_{k+1}$. This approximation is given by the second equation above. To get the third equation we must use the function $f$ and use the value of $t$ to approximate the value between $u_{k}$ and $u_{k+1}$. This gives us an approximation at the midpoint and then that is multiplied by the change in t to give us the point we are looking for. This is more accurate because it is not over estimating or underestimating as largely as before, since it is taking the midpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. 4th-order Runge-Kutta Method (accurate to 4th-order):\n",
    "   \n",
    "   $u_{k+1} = u_k + (K_1 + 2K_2 + 2K_3 + K_4)/6$, \n",
    "   \n",
    "   $K_1 = \\Delta t\\,f[t_k,u_k]$, \n",
    "   \n",
    "   $K_2 = \\Delta t\\, f[t_k + \\Delta t/2, u_k + K_1/2]$, \n",
    "   \n",
    "   $K_3 = \\Delta t\\, f[t_k + \\Delta t/2, u_k + K_2/2]$, \n",
    "   \n",
    "   $K_4 = \\Delta t\\,f[t_k + \\Delta t, u_k + K_3]$  \n",
    "   \n",
    "This one uses the four points given from these 4 equations to approximate a point (this is the most accurate). The first point is the approximation of the difference between $u_{k+1}$ and $u_{k}$. The second point is the approximation using the middle value of $t$ and the same two values. The third point approximates using the same middle $t$ value, as well as the gradient from the second equation. The fourth point we get a slope of the point in the third equation and follow this line from $t$ to $t$ + the change in $t$. To get the point we average all these values and then we are given our approximation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 5)",
   "language": "python",
   "name": "anaconda5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}